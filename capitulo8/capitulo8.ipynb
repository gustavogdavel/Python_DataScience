{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264becda",
   "metadata": {},
   "source": [
    "# Capítulo 8 - Modelando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c3f7e",
   "metadata": {},
   "source": [
    "## Trabalhando com Páginas HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc9e3b",
   "metadata": {},
   "source": [
    "### Analisando XML e HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a352c8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'bool'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\55279\\AppData\\Local\\Temp\\ipykernel_4572\\1208396869.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row_s)\n",
      "C:\\Users\\55279\\AppData\\Local\\Temp\\ipykernel_4572\\1208396869.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row_s)\n",
      "C:\\Users\\55279\\AppData\\Local\\Temp\\ipykernel_4572\\1208396869.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row_s)\n",
      "C:\\Users\\55279\\AppData\\Local\\Temp\\ipykernel_4572\\1208396869.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row_s)\n"
     ]
    }
   ],
   "source": [
    "from lxml import objectify\n",
    "import pandas as pd\n",
    "from distutils import util\n",
    "\n",
    "xml = objectify.parse(open('XMLData.xml'))\n",
    "root = xml.getroot()\n",
    "df = pd.DataFrame(columns = ('Number', 'Boolean'))\n",
    "\n",
    "for i in range(0,4):\n",
    "    obj = root.getchildren()[i].getchildren()\n",
    "    row = dict(zip(['Number', 'Boolean'],\n",
    "                  [obj[0].pyval,\n",
    "                  bool(util.strtobool(obj[2].text))]))\n",
    "    row_s = pd.Series(row)\n",
    "    row_s.name = obj[1].text\n",
    "    df = df.append(row_s)\n",
    "    \n",
    "print(type(df.loc['First']['Number']))\n",
    "print(type(df.loc['First']['Boolean']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472161df",
   "metadata": {},
   "source": [
    "Este código tem como objetivo ler um arquivo XML, extrair informações específicas e armazená-las em um dataframe do Pandas. Ele utiliza a biblioteca lxml para analisar o arquivo XML e a biblioteca Pandas para armazenar as informações em um formato de tabela. O código extrai informações numéricas e booleanas de cada nó do arquivo XML e as armazena em um dataframe com duas colunas, \"Number\" e \"Boolean\". O objetivo final é permitir que as informações do arquivo XML sejam manipuladas e analisadas com facilidade usando a biblioteca Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959227a",
   "metadata": {},
   "source": [
    "### Usando o XPath para extração de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2dcb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Number  Boolean\n",
      "First        1     True\n",
      "Second       2    False\n",
      "Third        3     True\n",
      "Fourth       4    False\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.bool_'>\n"
     ]
    }
   ],
   "source": [
    "from lxml import objectify\n",
    "import pandas as pd\n",
    "from distutils import util\n",
    "\n",
    "xml = objectify.parse(open('XMLData.xml'))\n",
    "root = xml.getroot()\n",
    "\n",
    "map_number = map(int, root.xpath('Record/Number'))\n",
    "map_bool = map(str, root.xpath('Record/Boolean'))\n",
    "map_bool = map(util.strtobool, map_bool)\n",
    "map_bool = map(bool, map_bool)\n",
    "map_string = map(str, root.xpath('Record/String'))\n",
    "\n",
    "data = list(zip(map_number, map_bool))\n",
    "\n",
    "df = pd.DataFrame(data,\n",
    "                 columns=('Number', 'Boolean'),\n",
    "                 index = list(map_string))\n",
    "\n",
    "print(df)\n",
    "print(type(df.loc['First']['Number']))\n",
    "print(type(df.loc['First']['Boolean']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591cd2b",
   "metadata": {},
   "source": [
    "## Trabalhando com Texto Puro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066488ba",
   "metadata": {},
   "source": [
    "### Lidando com Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd94b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lembre de sempre usar UTF-8 ao trabalhar com Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095c634",
   "metadata": {},
   "source": [
    "### Stemizando e removendo palavras vazias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcecb20",
   "metadata": {},
   "source": [
    "Stemização é o processo de reduzir uma palavra ao seu radical, removendo sufixos e prefixos, a fim de obter sua forma básica. O objetivo é obter uma forma canônica para palavras relacionadas, facilitando a comparação e a busca de informações em textos.\n",
    "\n",
    "Ex: gato, gata, gatinho serão reduzidos para o radical \"gat\"\n",
    "\n",
    "É uma técnica fundamental em Data Science para pré-processamento de dados de texto em NLP, que ajuda a tornar o processo de análise de dados mais preciso, eficiente e escalável."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a95ee",
   "metadata": {},
   "source": [
    "## Usando o Modelo Saco de Palavras e Mais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b38575",
   "metadata": {},
   "source": [
    "Técnica de processamento de linguagem natural que consiste em representar um texto como um conjunto de palavras, sem levar em consideração a ordem em que elas aparecem. Cada palavra é tratada como uma \"feature\" ou variável e a frequência de cada palavra no texto é utilizada para criar um vetor de características que representa o texto. Esse modelo é usado em tarefas como classificação de textos e mineração de sentimentos, entre outras aplicações.\n",
    "\n",
    "Antes de aplicar o modelo saco de palavras, é comum realizar algumas práticas de pré-processamento de texto, como:\n",
    "\n",
    "1- Remover caracteres indesejados, como pontuação, números, caracteres especiais, etc.\n",
    "\n",
    "2- Converter todo o texto para minúsculas (ou maiúsculas) para que palavras iguais, mas com letras maiúsculas ou minúsculas diferentes, sejam tratadas como iguais.\n",
    "\n",
    "3- Remover palavras irrelevantes (stopwords), que são palavras comuns que não adicionam nenhum valor semântico ao texto, como \"a\", \"o\", \"de\", \"para\", etc.\n",
    "\n",
    "4- Realizar a lematização ou a stemização das palavras, que é a redução das palavras para a sua forma básica (radical), de forma a agrupar palavras semelhantes.\n",
    "\n",
    "5- Realizar a tokenização do texto, que é a divisão do texto em tokens (palavras) individuais para que possam ser contados e representados pelo modelo saco de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e933aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW shape: (2356, 34750)\n",
      "\"Caltech\": 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "categories = ['comp.graphics', 'misc.forsale',\n",
    "             'rec.autos', 'sci.space']\n",
    "twenty_train = fetch_20newsgroups(subset = 'train',  #coleção de aproximadamente 20.000 documentos de grupos de discussão Usenet\n",
    "                                 categories=categories,\n",
    "                                 shuffle=True,\n",
    "                                 random_state=42)\n",
    "\n",
    "count_vect = CountVectorizer()                 # transforma os documentos de texto em uma matriz de frequência de palavras.\n",
    "X_train_counts = count_vect.fit_transform(\n",
    "    twenty_train.data)\n",
    "\n",
    "print(\"BOW shape:\", X_train_counts.shape)\n",
    "caltech_idx = count_vect.vocabulary_['caltech']\n",
    "print('\"Caltech\": %i' % X_train_counts[0, caltech_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b608d",
   "metadata": {},
   "source": [
    "Em resumo, o código carrega os dados de treinamento do conjunto de dados 20 Newsgroups, constrói uma matriz de frequência de palavras usando o CountVectorizer e exibe a forma da matriz resultante, bem como a contagem de ocorrências da palavra \"caltech\" no primeiro documento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869937e",
   "metadata": {},
   "source": [
    "## Trabalhando com n-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4f8b8",
   "metadata": {},
   "source": [
    "Em processamento de linguagem natural, n-gramas são sequências contíguas de n elementos (geralmente palavras) em um texto. Os n-gramas são usados para análise estatística e modelagem de linguagem, permitindo extrair informações sobre a estrutura e o contexto dos textos. Por exemplo, um 2-grama (ou bigrama) em um texto como \"O gato preto\" seria: \"O gato\", \"gato preto\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b97814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' an' ' in' ' of' ' th' ' to' 'he ' 'ing' 'ion' 'nd ' 'the']\n",
      "[[0 0 2 5 1 4 2 2 0 5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "categories = ['sci.space']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                 categories=categories,\n",
    "                                 remove=('headers',\n",
    "                                         'footers',\n",
    "                                         'quotes'),   # remover cabeçalhos, rodapés e citações dos documentos\n",
    "                                 shuffle=True,\n",
    "                                 random_state=42)\n",
    "\n",
    "count_chars = CountVectorizer(analyzer='char_wb',\n",
    "                              ngram_range=(3,3),\n",
    "                              max_features=10)\n",
    "\n",
    "X = count_chars.fit_transform(twenty_train.data)\n",
    "\n",
    "feature_names = count_chars.get_feature_names_out()\n",
    "\n",
    "print(feature_names)\n",
    "print(X[1].todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8968b26",
   "metadata": {},
   "source": [
    "Em resumo, o código analisa os textos do conjunto de dados, extrai os n-gramas de caracteres e palavras mais frequentes e exibe essas informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b20f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
